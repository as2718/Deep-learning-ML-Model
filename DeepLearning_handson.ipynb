{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acadc038",
   "metadata": {},
   "source": [
    "The decision boundary of each output neuron is linear, so Perceptrons are incapable of learning complex patterns(just like Logistic classifiers).However, if the training instances are linearly separable,Rosenblatt demonstrated that this algorithm would converge to a solution.This is called the Perceptron Convergence Theorem.\n",
    "\n",
    "Scikit-Learn provides a Perceptron class that implements a single-TLU network.It can be used pretty much as you would expect-for example, on the iris datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23840003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:,(2,3)] # petal length and width\n",
    "y = (iris.target == 0).astype(int) # Iris setosa?\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X,y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b829d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9518b3",
   "metadata": {},
   "source": [
    "# Forecasting a Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "108334ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(batch_size, n_steps):\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20))\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)\n",
    "    return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83742685",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps= 50\n",
    "series = generate_time_series(10000, n_steps+1)\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "X_valid, y_valid = series[7000:9000,:n_steps], series[7000:9000, -1]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddccc886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6243de42",
   "metadata": {},
   "source": [
    "## Baseline Matrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a200b817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021079862"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "y_pred = X_valid[:, -1]\n",
    "np.mean(keras.losses.mean_squared_error(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f605c054",
   "metadata": {},
   "source": [
    "Another simple approach is to use a fully conncted network.Since it expects a flat list of features for each input, we need to add a Flatten layer.Lets just use a simple Linear Regression model so that each prediction will be a linear combination of the values in the times series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d60ec6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Abhishek Srivastav\\Anaconda\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[50, 1]),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7292908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.64664835],\n",
       "        [-0.6314309 ],\n",
       "        [-0.5818596 ],\n",
       "        ...,\n",
       "        [ 0.0177887 ],\n",
       "        [-0.1094785 ],\n",
       "        [-0.12598152]],\n",
       "\n",
       "       [[ 0.18005867],\n",
       "        [-0.07718117],\n",
       "        [-0.23642164],\n",
       "        ...,\n",
       "        [-0.2560669 ],\n",
       "        [-0.27271163],\n",
       "        [-0.25928968]],\n",
       "\n",
       "       [[-0.05538947],\n",
       "        [-0.07547172],\n",
       "        [-0.09227587],\n",
       "        ...,\n",
       "        [-0.27734149],\n",
       "        [-0.14546561],\n",
       "        [-0.10853981]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.25889802],\n",
       "        [ 0.28087577],\n",
       "        [ 0.32635367],\n",
       "        ...,\n",
       "        [ 0.61782104],\n",
       "        [ 0.51009524],\n",
       "        [ 0.3705604 ]],\n",
       "\n",
       "       [[-0.4888316 ],\n",
       "        [-0.38419583],\n",
       "        [-0.22218531],\n",
       "        ...,\n",
       "        [-0.29471216],\n",
       "        [-0.17163214],\n",
       "        [ 0.00397158]],\n",
       "\n",
       "       [[ 0.25146055],\n",
       "        [ 0.37155056],\n",
       "        [ 0.48482686],\n",
       "        ...,\n",
       "        [-0.37325314],\n",
       "        [-0.4285371 ],\n",
       "        [-0.45727104]]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ee90aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Abhishek Srivastav\\Anaconda\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc00cee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\Users\\Abhishek Srivastav\\Anaconda\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "219/219 [==============================] - 4s 7ms/step - loss: 0.0868 - val_loss: 0.0458\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0313 - val_loss: 0.0221\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0163 - val_loss: 0.0128\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0107 - val_loss: 0.0092\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0084 - val_loss: 0.0076\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0072 - val_loss: 0.0066\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0064 - val_loss: 0.0060\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0058 - val_loss: 0.0054\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0053 - val_loss: 0.0049\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0049 - val_loss: 0.0047\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0046 - val_loss: 0.0043\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0043 - val_loss: 0.0042\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0036 - val_loss: 0.0037\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c06f5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0037\n",
      "Mean Squared Error on the validation set: 0.0037384419701993465\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model on the validation set and calculate MSE\n",
    "mse = model.evaluate(X_valid, y_valid)\n",
    "print(f'Mean Squared Error on the validation set: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fbc38d",
   "metadata": {},
   "source": [
    "## Implementing a simple RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b3f2d4",
   "metadata": {},
   "source": [
    "lets see if we can beat that with a simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6f55581",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af11bce4",
   "metadata": {},
   "source": [
    "This is simplest RNN which contains a single layer with a single neuron, we do not need to specify the length of the input sequences since a recurrent neural network can process any number of time steps.By Default, the simpleRNN layers uses the hyperbolic tangent activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "539aadd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fad398c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 7s 21ms/step - loss: 0.1674 - val_loss: 0.1500\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 4s 17ms/step - loss: 0.1381 - val_loss: 0.1246\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 4s 17ms/step - loss: 0.1125 - val_loss: 0.1001\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 4s 17ms/step - loss: 0.0890 - val_loss: 0.0785\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 4s 18ms/step - loss: 0.0685 - val_loss: 0.0595\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 4s 17ms/step - loss: 0.0508 - val_loss: 0.0432\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 4s 18ms/step - loss: 0.0360 - val_loss: 0.0304\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 4s 17ms/step - loss: 0.0251 - val_loss: 0.0216\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 4s 17ms/step - loss: 0.0183 - val_loss: 0.0166\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 4s 17ms/step - loss: 0.0149 - val_loss: 0.0145\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 4s 17ms/step - loss: 0.0136 - val_loss: 0.0137\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 4s 17ms/step - loss: 0.0130 - val_loss: 0.0133\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 4s 17ms/step - loss: 0.0127 - val_loss: 0.0130\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 4s 17ms/step - loss: 0.0125 - val_loss: 0.0127\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 4s 17ms/step - loss: 0.0123 - val_loss: 0.0125\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 4s 17ms/step - loss: 0.0121 - val_loss: 0.0123\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 4s 18ms/step - loss: 0.0119 - val_loss: 0.0121\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 4s 17ms/step - loss: 0.0117 - val_loss: 0.0119\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 4s 17ms/step - loss: 0.0115 - val_loss: 0.0117\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 4s 17ms/step - loss: 0.0114 - val_loss: 0.0117\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b80ed29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 8ms/step - loss: 0.0117\n",
      "Mean Squared Error on the validation set: 0.011650647968053818\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set and calculate MSE\n",
    "mse = model.evaluate(X_valid, y_valid)\n",
    "print(f'Mean Squared Error on the validation set: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c64fe",
   "metadata": {},
   "source": [
    "## Deep RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46d27d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None,1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4079a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90cf066c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 19s 52ms/step - loss: 0.0147 - val_loss: 0.0046\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 10s 44ms/step - loss: 0.0039 - val_loss: 0.0033\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 9s 43ms/step - loss: 0.0034 - val_loss: 0.0031\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 10s 45ms/step - loss: 0.0031 - val_loss: 0.0028\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 10s 44ms/step - loss: 0.0030 - val_loss: 0.0032\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 10s 45ms/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 10s 44ms/step - loss: 0.0029 - val_loss: 0.0027\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 10s 45ms/step - loss: 0.0029 - val_loss: 0.0026\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 9s 43ms/step - loss: 0.0028 - val_loss: 0.0026\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 9s 43ms/step - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 10s 44ms/step - loss: 0.0028 - val_loss: 0.0039\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 10s 44ms/step - loss: 0.0028 - val_loss: 0.0025\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 9s 43ms/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 9s 43ms/step - loss: 0.0027 - val_loss: 0.0025\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 10s 44ms/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 10s 45ms/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 10s 45ms/step - loss: 0.0026 - val_loss: 0.0024\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 10s 45ms/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 10s 45ms/step - loss: 0.0027 - val_loss: 0.0025\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 13s 57ms/step - loss: 0.0027 - val_loss: 0.0025\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b29be616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 16ms/step - loss: 0.0025\n",
      "Mean Squared Error on the validation set: 0.002548994030803442\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set and calculate MSE\n",
    "mse = model.evaluate(X_valid, y_valid)\n",
    "print(f'Mean Squared Error on the validation set: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62710f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None,1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc2debef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ea6a7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 17s 46ms/step - loss: 0.1437 - val_loss: 0.1274\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 9s 40ms/step - loss: 0.1250 - val_loss: 0.1177\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 8s 35ms/step - loss: 0.1140 - val_loss: 0.1045\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 4s 18ms/step - loss: 0.1027 - val_loss: 0.0979\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 8s 34ms/step - loss: 0.0964 - val_loss: 0.0956\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 8s 35ms/step - loss: 0.0926 - val_loss: 0.0903\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 7s 32ms/step - loss: 0.0897 - val_loss: 0.0853\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 7s 34ms/step - loss: 0.0888 - val_loss: 0.0905\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 7s 32ms/step - loss: 0.0884 - val_loss: 0.0849\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 7s 34ms/step - loss: 0.0862 - val_loss: 0.0830\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 7s 33ms/step - loss: 0.0848 - val_loss: 0.0833\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 7s 34ms/step - loss: 0.0831 - val_loss: 0.0812\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 7s 34ms/step - loss: 0.0828 - val_loss: 0.0804\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 7s 33ms/step - loss: 0.0809 - val_loss: 0.0833\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 7s 33ms/step - loss: 0.0805 - val_loss: 0.0786\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 8s 34ms/step - loss: 0.0794 - val_loss: 0.0793\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 7s 33ms/step - loss: 0.0787 - val_loss: 0.0771\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 7s 34ms/step - loss: 0.0781 - val_loss: 0.0753\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 8s 35ms/step - loss: 0.0772 - val_loss: 0.0788\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 7s 34ms/step - loss: 0.0769 - val_loss: 0.0781\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0367a011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 14ms/step - loss: 0.0781\n",
      "Mean Squared Error on the validation set: 0.07814665883779526\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set and calculate MSE\n",
    "mse = model.evaluate(X_valid, y_valid)\n",
    "print(f'Mean Squared Error on the validation set: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1c76d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 889ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 2, the array at index 0 has size 1 and the array at index 1 has size 20",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step_ahead \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      5\u001b[0m     y_pred_one \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X[:, step_ahead:])[:, np\u001b[38;5;241m.\u001b[39mnewaxis, :]\n\u001b[1;32m----> 6\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([X, y_pred_one], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m X[:, n_steps:]\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 2, the array at index 0 has size 1 and the array at index 1 has size 20"
     ]
    }
   ],
   "source": [
    "series = generate_time_series(1,n_steps+10)\n",
    "X_new, Y_new = series[:, :n_steps],series[:, n_steps:]\n",
    "X = X_new\n",
    "for step_ahead in range(10):\n",
    "    y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :]\n",
    "    x = np.concatenate([X, y_pred_one], axis=1)\n",
    "    \n",
    "Y_pred = X[:, n_steps:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075e65ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7788bbd7",
   "metadata": {},
   "source": [
    "The Second option is to train an RNN to predict all 10 next values at once.We can still use a sequence-to-vector model, but it will output 10 values instead of 1.However, we first need to change the targets to be vectors containig the next 10 values\n",
    ":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4f2f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_time_series(10000, n_steps + 10)\n",
    "X_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
    "X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\n",
    "X_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac545e7",
   "metadata": {},
   "source": [
    "Now we just need the output layer to have 10 units instead of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50b0f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(20)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15772c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 61ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred=model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f9fa717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.61012095,  0.44307926, -0.2140224 , -0.51406616,  0.04858202,\n",
       "         0.3525835 , -0.22086477, -0.10937619, -0.19917727, -0.38751072,\n",
       "         0.48200843, -0.11143138, -0.01095603,  0.96937877, -0.17860182,\n",
       "         0.26824927, -0.20585363,  0.8874372 ,  0.16062392, -0.29658255]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea4ba20",
   "metadata": {},
   "source": [
    "Instead of traning the model to forecast 10 values only at the very last step , we can train it to forecast the next 10 values at each and every time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff84290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.empty((10000, n_steps, 10))\n",
    "for step_ahead in range(1, 10 +  1):\n",
    "    Y[:, :, step_ahead - 1] = series[:, step_ahead:step_ahead + n_steps,0]\n",
    "\n",
    "Y_train = Y[:7000]\n",
    "Y_valid = Y[7000:9000]\n",
    "Y_test = Y[9000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b5c5a6",
   "metadata": {},
   "source": [
    "To turn the model into a sequence-to-sequence model, we must set return_sequences=True in all recurrent layers(even the last one), and we must apply the output Dense layer at every time step.Keras offers a TimeDistributed layer for this very purpose , it wraps any layer (eg Dense layer) and applies it at every time step of its input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c12daa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10)) # makes the dense layer to be applied independently at each time step and that the model will output a sequence, not just a single vector\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae713ea",
   "metadata": {},
   "source": [
    "All outputs are  needed during training , but only the output at the last time step is useful for predictions ad for evaluation. So although we will rely on the MSE over all the outputs for training , we will use a custom metric for evaluation, to only compute the MSE over the output at the last time step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ad47887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "def last_time_step_mse(Y_true, Y_pred):\n",
    "    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[last_time_step_mse])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c4a0ac",
   "metadata": {},
   "source": [
    "# Fighting the Unstable Gradients Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdb7d4c",
   "metadata": {},
   "source": [
    "Let's use tf.keras to implement Layer Normalization within a simple memory cell.For this , we need to define a custom memory cell.It is juts like a regular layer ,except it call() method takes two arguments : the inputs amd the current time stepamd hidden states from the previous time step.Note that the states argument is a list containig equals to the outputs of the previous time step, but other cells may multiple state tensors (e.g., an LSTMCell has a long term state and a short term state as we see shortly).A cell must have a state_size attribute and an output_size attibute.In a simple RNN,both are simply equal to the number of units.\n",
    "\n",
    "The following code implements a custom memory cell which will behave like SimpleRNNCell except it will also apply Layer Normalization at each time step:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9850d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LNSimpleRNNCell(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.state_size = units\n",
    "        self.output_size = units\n",
    "        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units,activation=None)\n",
    "        self.layer_norm = keras.layers.LayerNormalization()\n",
    "        self.activation = keras.activations.get(activation)\n",
    "    def call(self, inputs, states):\n",
    "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
    "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
    "        return norm_outputs, [norm_outputs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af33676",
   "metadata": {},
   "source": [
    "Above code , LNSimpleRNNCell class inherits from keras.layers.Layer class , just like any custom layer.The constructor takes the number of units and the desired activation function and sets the state_size and output_size attributes , then creates a SimpleRNNCell with no activation function(because we want to perform Layer Normalization after the linear operation but before the activation function). Then the constructor creates the LayerNormaization layer, and finally it fetches the desired activation function.The call() method starts by applying the simple RNN cell, which computes a linear combination of the current inputs and the previous hidden states, and it returns the result twice(indeed in a SimpleRNNCell, the outputs are just equal to the hidden states: in other words,new_states[0] is equal to outputs, so we can safely ignore new_states in the rest of the call() method).Next, the call() method applies Layer Normalization,followed by the activation function.Finally it returns the output twice(once as the outputs and once as the new hidden states).To use this custom cell, all we need to do is create a keras.layers.RNN layer, passing it a cell instance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d9f53eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Abhishek Srivastav\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\normalization\\layer_normalization.py:328: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Abhishek Srivastav\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\normalization\\layer_normalization.py:328: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe75593",
   "metadata": {},
   "source": [
    "Similarly, you could createa a custom cell to apply dropout between each time step.But there's a simple way: all recurrent layers(except for keras.layers.RNN) and all cells provided by Keras has a droput parameter and a recurrent_dropout hyper-parameter: the former defines the dropout rate for the hidden states(also at each time step).No need to create a custom cell to apply dropout at each time step in an RNN.\n",
    "With these techniques, you can alleviate the unstable gardients problem and train an RNN much more efficiently.Now lets look at how to deal with the short-term memory problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5602e",
   "metadata": {},
   "source": [
    "# Tackling the Short-Term Memory Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f2811",
   "metadata": {},
   "source": [
    "In Keras, you can simply use the LSTM layer instead of SimpleRNN layer to make a RNN to LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50461223",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.LSTM(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879bd8ce",
   "metadata": {},
   "source": [
    "However, the LSTM layer uses an optimised implementation when running on GPU , so it is peferable to use it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c6d692",
   "metadata": {},
   "source": [
    "# Apply 1D convolutional layer to GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9a5e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv1D(filters=20,kernel_size=4,strides=2,padding=\"valid\" ,input_shape=[None, 1]),\n",
    "    keras.layers.GRU(20, return_sequences=True),\n",
    "    keras.layers.GRU(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18e24a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam',metrics=[last_time_step_mse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e7d17026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\Users\\Abhishek Srivastav\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Abhishek Srivastav\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 28s 58ms/step - loss: 0.0740 - last_time_step_mse: 0.0862 - val_loss: 0.0147 - val_last_time_step_mse: 0.0339\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 10s 44ms/step - loss: 0.0084 - last_time_step_mse: 0.0223 - val_loss: 0.0023 - val_last_time_step_mse: 0.0045\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 10s 45ms/step - loss: 0.0012 - last_time_step_mse: 0.0015 - val_loss: 7.2671e-04 - val_last_time_step_mse: 3.9077e-04\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 10s 46ms/step - loss: 6.3668e-04 - last_time_step_mse: 2.6726e-04 - val_loss: 5.6409e-04 - val_last_time_step_mse: 1.7073e-04\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 10s 46ms/step - loss: 5.3415e-04 - last_time_step_mse: 1.7157e-04 - val_loss: 5.0060e-04 - val_last_time_step_mse: 1.7627e-04\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 10s 44ms/step - loss: 4.8129e-04 - last_time_step_mse: 1.3396e-04 - val_loss: 4.8730e-04 - val_last_time_step_mse: 1.1001e-04\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 9s 43ms/step - loss: 4.4720e-04 - last_time_step_mse: 1.1730e-04 - val_loss: 4.2142e-04 - val_last_time_step_mse: 9.8816e-05\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 9s 42ms/step - loss: 4.0983e-04 - last_time_step_mse: 8.0912e-05 - val_loss: 3.8917e-04 - val_last_time_step_mse: 5.5366e-05\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 10s 44ms/step - loss: 3.8992e-04 - last_time_step_mse: 9.5458e-05 - val_loss: 3.6461e-04 - val_last_time_step_mse: 4.9230e-05\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 10s 44ms/step - loss: 3.5100e-04 - last_time_step_mse: 5.7467e-05 - val_loss: 3.3485e-04 - val_last_time_step_mse: 5.6301e-05\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 10s 46ms/step - loss: 3.3418e-04 - last_time_step_mse: 8.9477e-05 - val_loss: 3.1800e-04 - val_last_time_step_mse: 7.3134e-05\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 10s 45ms/step - loss: 2.9147e-04 - last_time_step_mse: 5.0754e-05 - val_loss: 2.8307e-04 - val_last_time_step_mse: 6.2272e-05\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 9s 43ms/step - loss: 2.6510e-04 - last_time_step_mse: 6.9025e-05 - val_loss: 2.3439e-04 - val_last_time_step_mse: 2.7460e-05\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 9s 43ms/step - loss: 2.1770e-04 - last_time_step_mse: 3.5526e-05 - val_loss: 1.9328e-04 - val_last_time_step_mse: 2.2246e-05\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 10s 44ms/step - loss: 1.7990e-04 - last_time_step_mse: 4.6019e-05 - val_loss: 1.4849e-04 - val_last_time_step_mse: 1.6564e-05\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 9s 43ms/step - loss: 1.4683e-04 - last_time_step_mse: 7.4291e-05 - val_loss: 1.0727e-04 - val_last_time_step_mse: 1.1103e-05\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 9s 43ms/step - loss: 9.1463e-05 - last_time_step_mse: 1.3598e-05 - val_loss: 7.3897e-05 - val_last_time_step_mse: 6.8412e-06\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 9s 43ms/step - loss: 8.4920e-05 - last_time_step_mse: 7.4624e-05 - val_loss: 5.2634e-05 - val_last_time_step_mse: 5.9568e-06\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 9s 42ms/step - loss: 4.5880e-05 - last_time_step_mse: 6.7255e-06 - val_loss: 4.0200e-05 - val_last_time_step_mse: 1.0251e-05\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 9s 41ms/step - loss: 3.8703e-05 - last_time_step_mse: 2.0625e-05 - val_loss: 1.3081e-04 - val_last_time_step_mse: 3.3685e-04\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,Y_train[:,3::2],epochs=20,validation_data=(X_valid, Y_valid[:, 3::2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2bcf0638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 0.4128169 ],\n",
       "         [ 0.5113632 ],\n",
       "         [ 0.59807044],\n",
       "         ...,\n",
       "         [ 0.19303648],\n",
       "         [ 0.17872018],\n",
       "         [ 0.08321963]],\n",
       " \n",
       "        [[-0.46015626],\n",
       "         [-0.46750906],\n",
       "         [-0.44173956],\n",
       "         ...,\n",
       "         [ 0.49921614],\n",
       "         [ 0.4350306 ],\n",
       "         [ 0.37085634]],\n",
       " \n",
       "        [[-0.50013334],\n",
       "         [-0.61801845],\n",
       "         [-0.62538296],\n",
       "         ...,\n",
       "         [ 0.28145257],\n",
       "         [ 0.33534035],\n",
       "         [ 0.35960633]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.56406534],\n",
       "         [-0.60747445],\n",
       "         [-0.6084812 ],\n",
       "         ...,\n",
       "         [ 0.25958344],\n",
       "         [ 0.30402866],\n",
       "         [ 0.2100297 ]],\n",
       " \n",
       "        [[-0.29693457],\n",
       "         [-0.2686142 ],\n",
       "         [-0.28101456],\n",
       "         ...,\n",
       "         [-0.58783597],\n",
       "         [-0.66277367],\n",
       "         [-0.6597046 ]],\n",
       " \n",
       "        [[-0.2669471 ],\n",
       "         [-0.38518706],\n",
       "         [-0.4471798 ],\n",
       "         ...,\n",
       "         [ 0.10362528],\n",
       "         [ 0.01292876],\n",
       "         [-0.01603315]]], dtype=float32),\n",
       " array([[[ 0.25004855,  0.41041344,  0.44373941, ...,  0.15305746,\n",
       "           0.08614351,  0.10032348],\n",
       "         [ 0.44373941,  0.40650484,  0.35171744, ...,  0.10032348,\n",
       "          -0.05922982, -0.13141029],\n",
       "         [ 0.35171744,  0.32504067,  0.19129938, ..., -0.13141029,\n",
       "          -0.26867238, -0.39145887],\n",
       "         ...,\n",
       "         [ 0.66454065,  0.57003385,  0.43595228, ..., -0.66078562,\n",
       "          -0.58927327, -0.49969879],\n",
       "         [ 0.43595228,  0.24502313, -0.08187151, ..., -0.49969879,\n",
       "          -0.33180842, -0.16870072],\n",
       "         [-0.08187151, -0.30341136, -0.53374064, ..., -0.16870072,\n",
       "          -0.00089615,  0.14663929]],\n",
       " \n",
       "        [[ 0.25004855,  0.41041344,  0.44373941, ...,  0.15305746,\n",
       "           0.08614351,  0.10032348],\n",
       "         [ 0.44373941,  0.40650484,  0.35171744, ...,  0.10032348,\n",
       "          -0.05922982, -0.13141029],\n",
       "         [ 0.35171744,  0.32504067,  0.19129938, ..., -0.13141029,\n",
       "          -0.26867238, -0.39145887],\n",
       "         ...,\n",
       "         [ 0.66454065,  0.57003385,  0.43595228, ..., -0.66078562,\n",
       "          -0.58927327, -0.49969879],\n",
       "         [ 0.43595228,  0.24502313, -0.08187151, ..., -0.49969879,\n",
       "          -0.33180842, -0.16870072],\n",
       "         [-0.08187151, -0.30341136, -0.53374064, ..., -0.16870072,\n",
       "          -0.00089615,  0.14663929]],\n",
       " \n",
       "        [[ 0.25004855,  0.41041344,  0.44373941, ...,  0.15305746,\n",
       "           0.08614351,  0.10032348],\n",
       "         [ 0.44373941,  0.40650484,  0.35171744, ...,  0.10032348,\n",
       "          -0.05922982, -0.13141029],\n",
       "         [ 0.35171744,  0.32504067,  0.19129938, ..., -0.13141029,\n",
       "          -0.26867238, -0.39145887],\n",
       "         ...,\n",
       "         [ 0.66454065,  0.57003385,  0.43595228, ..., -0.66078562,\n",
       "          -0.58927327, -0.49969879],\n",
       "         [ 0.43595228,  0.24502313, -0.08187151, ..., -0.49969879,\n",
       "          -0.33180842, -0.16870072],\n",
       "         [-0.08187151, -0.30341136, -0.53374064, ..., -0.16870072,\n",
       "          -0.00089615,  0.14663929]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.25004855,  0.41041344,  0.44373941, ...,  0.15305746,\n",
       "           0.08614351,  0.10032348],\n",
       "         [ 0.44373941,  0.40650484,  0.35171744, ...,  0.10032348,\n",
       "          -0.05922982, -0.13141029],\n",
       "         [ 0.35171744,  0.32504067,  0.19129938, ..., -0.13141029,\n",
       "          -0.26867238, -0.39145887],\n",
       "         ...,\n",
       "         [ 0.66454065,  0.57003385,  0.43595228, ..., -0.66078562,\n",
       "          -0.58927327, -0.49969879],\n",
       "         [ 0.43595228,  0.24502313, -0.08187151, ..., -0.49969879,\n",
       "          -0.33180842, -0.16870072],\n",
       "         [-0.08187151, -0.30341136, -0.53374064, ..., -0.16870072,\n",
       "          -0.00089615,  0.14663929]],\n",
       " \n",
       "        [[ 0.25004855,  0.41041344,  0.44373941, ...,  0.15305746,\n",
       "           0.08614351,  0.10032348],\n",
       "         [ 0.44373941,  0.40650484,  0.35171744, ...,  0.10032348,\n",
       "          -0.05922982, -0.13141029],\n",
       "         [ 0.35171744,  0.32504067,  0.19129938, ..., -0.13141029,\n",
       "          -0.26867238, -0.39145887],\n",
       "         ...,\n",
       "         [ 0.66454065,  0.57003385,  0.43595228, ..., -0.66078562,\n",
       "          -0.58927327, -0.49969879],\n",
       "         [ 0.43595228,  0.24502313, -0.08187151, ..., -0.49969879,\n",
       "          -0.33180842, -0.16870072],\n",
       "         [-0.08187151, -0.30341136, -0.53374064, ..., -0.16870072,\n",
       "          -0.00089615,  0.14663929]],\n",
       " \n",
       "        [[ 0.25004855,  0.41041344,  0.44373941, ...,  0.15305746,\n",
       "           0.08614351,  0.10032348],\n",
       "         [ 0.44373941,  0.40650484,  0.35171744, ...,  0.10032348,\n",
       "          -0.05922982, -0.13141029],\n",
       "         [ 0.35171744,  0.32504067,  0.19129938, ..., -0.13141029,\n",
       "          -0.26867238, -0.39145887],\n",
       "         ...,\n",
       "         [ 0.66454065,  0.57003385,  0.43595228, ..., -0.66078562,\n",
       "          -0.58927327, -0.49969879],\n",
       "         [ 0.43595228,  0.24502313, -0.08187151, ..., -0.49969879,\n",
       "          -0.33180842, -0.16870072],\n",
       "         [-0.08187151, -0.30341136, -0.53374064, ..., -0.16870072,\n",
       "          -0.00089615,  0.14663929]]]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid, Y_valid[:, 3::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "940a92d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 12ms/step - loss: 1.3081e-04 - last_time_step_mse: 3.3685e-04\n",
      "Mean Squared Error on the validation set: [0.00013080639473628253, 0.0003368490724824369]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set and calculate MSE\n",
    "mse = model.evaluate(X_valid, Y_valid[:, 3::2])\n",
    "print(f'Mean Squared Error on the validation set: {mse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
